{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed057f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "llm = OllamaLLM(model=\"rnj-1:8b-cloud\", \n",
    "                max_tokens=1024,\n",
    "                temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd7b04",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "Output parsers take the output from an LLM and transform that output to a more suitable format. Parsing the output is very useful when you are using LLMs to generate any form of structured data, or to normalize output from chat models and other LLMs.\n",
    "\n",
    "LangChain has lots of different types of output parsers. This is a [list](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) of output parsers LangChain supports. In this lab, you will use the following two output parsers as examples:\n",
    "\n",
    "- `JSON`: Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.\n",
    "- `CSV`: Returns a list of comma separated values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ab320ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JsonOutputParser from langchain_core to convert LLM responses into structured JSON\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "# Import BaseModel and Field from pydantic\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dd2e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Netflix(BaseModel):\n",
    "    name : str = Field(description=\"Name of the Netflix show or movie\")\n",
    "    release_year : int = Field(description=\"Year the show or movie was released\")\n",
    "    genre : str = Field(description=\"Genre of the show or movie\")\n",
    "    mactor : str = Field(description=\"Main male actor in the show or movie\")\n",
    "    factor : str = Field(description=\"Main female actor in the show or movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48cc7648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('format_instructions: STRICT OUTPUT FORMAT:\\n'\n",
      " '- Return only the JSON value that conforms to the schema. Do not include any '\n",
      " 'additional text, explanations, headings, or separators.\\n'\n",
      " '- Do not wrap the JSON in Markdown or code fences (no ``` or ```json).\\n'\n",
      " '- Do not prepend or append any text (e.g., do not write \"Here is the '\n",
      " 'JSON:\").\\n'\n",
      " '- The response must be a single top-level JSON value exactly as required by '\n",
      " 'the schema (object/array/etc.), with no trailing commas or comments.\\n'\n",
      " '\\n'\n",
      " 'The output should be formatted as a JSON instance that conforms to the JSON '\n",
      " 'schema below.\\n'\n",
      " '\\n'\n",
      " 'As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", '\n",
      " '\"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": '\n",
      " '\"string\"}}}, \"required\": [\"foo\"]} the object {\"foo\": [\"bar\", \"baz\"]} is a '\n",
      " 'well-formatted instance of the schema. The object {\"properties\": {\"foo\": '\n",
      " '[\"bar\", \"baz\"]}} is not well-formatted.\\n'\n",
      " '\\n'\n",
      " 'Here is the output schema (shown in a code block for readability only — do '\n",
      " 'not include any backticks or Markdown in your output):\\n'\n",
      " '```\\n'\n",
      " '{\"properties\": {\"name\": {\"description\": \"Name of the Netflix show or movie\", '\n",
      " '\"title\": \"Name\", \"type\": \"string\"}, \"release_year\": {\"description\": \"Year '\n",
      " 'the show or movie was released\", \"title\": \"Release Year\", \"type\": '\n",
      " '\"integer\"}, \"genre\": {\"description\": \"Genre of the show or movie\", \"title\": '\n",
      " '\"Genre\", \"type\": \"string\"}, \"mactor\": {\"description\": \"Main male actor in '\n",
      " 'the show or movie\", \"title\": \"Mactor\", \"type\": \"string\"}, \"factor\": '\n",
      " '{\"description\": \"Main female actor in the show or movie\", \"title\": \"Factor\", '\n",
      " '\"type\": \"string\"}}, \"required\": [\"name\", \"release_year\", \"genre\", \"mactor\", '\n",
      " '\"factor\"]}\\n'\n",
      " '```')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'The Kissing Booth',\n",
       " 'release_year': 2018,\n",
       " 'genre': 'Romance',\n",
       " 'mactor': 'Jojo Siwa',\n",
       " 'factor': 'Taylor Zakhar Perez'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "netflix_query = \"Tell me about a Netflix show or movie top 1 in Thailand.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=Netflix)\n",
    "\n",
    "# Get the formatting instructions for the output parser\n",
    "# This generates guidance text that tells the LLM how to format its response\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "pprint(f\"format_instructions: {format_instructions}\")\n",
    "\n",
    "# Create a prompt template that includes:\n",
    "# 1. Instructions for the LLM to answer the user's query\n",
    "# 2. Format instructions to ensure the LLM returns properly structured data\n",
    "# 3. The actual user query placeholder\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\nFormat Instructions:{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],  # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\": format_instructions},  # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "# Create a processing chain that:\n",
    "# 1. Formats the prompt using the template\n",
    "# 2. Sends the formatted prompt to the Llama LLM\n",
    "# 3. Parses the LLM's response using the output parser to extract structured data\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain with a specific query about jokes\n",
    "# This will:\n",
    "# 1. Format the prompt with the joke query\n",
    "# 2. Send it to Llama\n",
    "# 3. Parse the response into the structure defined by your output parser\n",
    "# 4. Return the structured result\n",
    "chain.invoke({\"query\": netflix_query })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5acd4d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm unable to directly access real-time or specific regional data like Netflix's top 10 shows or movies in Thailand\",\n",
       " \"as my training only goes up until April 2023 and I don't have live internet access. However\",\n",
       " 'I can guide you on how to find this information:',\n",
       " '1. Visit the Netflix website or app.',\n",
       " '2. Set the language and region to Thailand.',\n",
       " '3. Browse or search for the top 10 shows or movies in Thailand.',\n",
       " \"If you have a specific list of shows or movies you're interested in\",\n",
       " 'I can provide more detailed information about them. Please let me know the names of the shows or movies',\n",
       " \"and I'll do my best to assist you.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "netflix_query = \"Tell me about a Netflix show or movie top 10 in Thailand.\"\n",
    "\n",
    "\n",
    "# Get the formatting instructions for the output parser\n",
    "# This generates guidance text that tells the LLM how to format its response\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt template that includes:\n",
    "# 1. Instructions for the LLM to answer the user's query\n",
    "# 2. Format instructions to ensure the LLM returns properly structured data\n",
    "# 3. The actual user query placeholder\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Answer the user query.\\nFormat Instructions:{format_instructions}\\n\n",
    "    colomn list    name     release_year    genre         mactor('Main male actor in the show or movie')     factor('Main female actor in the show or movie')  \n",
    "    {query}\\n\"\"\",\n",
    "    input_variables=[\"query\"],  # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\": format_instructions},  # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "# Create a processing chain that:\n",
    "# 1. Formats the prompt using the template\n",
    "# 2. Sends the formatted prompt to the Llama LLM\n",
    "# 3. Parses the LLM's response using the output parser to extract structured data\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain with a specific query about jokes\n",
    "# This will:\n",
    "# 1. Format the prompt with the joke query\n",
    "# 2. Send it to Llama\n",
    "# 3. Parse the response into the structure defined by your output parser\n",
    "# 4. Return the structured result\n",
    "chain.invoke({\"query\": netflix_query })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fdd4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fba09649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language.\\n Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a Document instance with:\n",
    "# 1. page_content: The actual text content about Python\n",
    "# 2. metadata: A dictionary containing additional information about this document\n",
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language.\n",
    " Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "metadata={\n",
    "    'my_document_id' : 234234,                      # Unique identifier for this document\n",
    "    'my_document_source' : \"About Python\",          # Source or title information\n",
    "    'my_document_create_time' : 1680013019          # Unix timestamp for document creation (March 28, 2023)\n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fb66e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c0f995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path= \"https://arxiv.org/pdf/2310.05421\",)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1011ce64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Microsoft® Word 2016',\n",
       " 'creator': 'Microsoft® Word 2016',\n",
       " 'creationdate': '2023-10-09T10:46:36+05:30',\n",
       " 'title': 'Paper Title (use style: paper title)',\n",
       " 'author': 'Keivalya Pandya;Dr Mehfuza Holia',\n",
       " 'moddate': '2023-10-09T10:46:36+05:30',\n",
       " 'source': 'https://arxiv.org/pdf/2310.05421',\n",
       " 'total_pages': 4,\n",
       " 'page': 1,\n",
       " 'page_label': '2'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bfe670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Submitted to the 3rd International Conference on “Women in Science & '\n",
      " 'Technology: Creating Sustainable Career”  \\n'\n",
      " '28 -30 December, 2023 \\n'\n",
      " 'III. METHODOLOGY \\n'\n",
      " 'This sect ion covers the data c ollection, details about the \\n'\n",
      " 'selected model, fine -tuning, and integration with the Gradio \\n'\n",
      " 'APIs for web deployment. \\n'\n",
      " 'A. Data Collection \\n'\n",
      " 'To gather the necessary data for our project, we employed \\n'\n",
      " 'BeautifulSoup web scraping techniques to retri eve publicly \\n'\n",
      " 'accessible information from an organization’s homepage. ')\n"
     ]
    }
   ],
   "source": [
    "pprint(document[1].page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93ad1953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easiest way to start building agents and a\n"
     ]
    }
   ],
   "source": [
    "# Import the WebBaseLoader class from langchain_community's document_loaders module\n",
    "# This loader is designed to scrape and extract text content from web pages\n",
    "from langchain_community.document_loaders import WebBaseLoader #use beautifulsoup4 libary for dealing with HTML\n",
    "\n",
    "# Create a WebBaseLoader instance by passing the URL of the web page to load\n",
    "# This URL points to the LangChain documentation's introduction page\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "\n",
    "# Call the load() method to:\n",
    "# 1. Send an HTTP request to the specified URL\n",
    "# 2. Download the HTML content\n",
    "# 3. Parse the HTML to extract meaningful text\n",
    "# 4. Create a list of Document objects containing the extracted content\n",
    "web_data = loader.load()\n",
    "\n",
    "# Print the first 1000 characters of the page content from the first Document\n",
    "# This provides a preview of the successfully loaded web content\n",
    "# web_data[0] accesses the first Document in the list\n",
    "# .page_content accesses the text content of that Document\n",
    "# [:1000] slices the string to get only the first 1000 characters\n",
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db99453",
   "metadata": {},
   "source": [
    "#### Text splitters\n",
    "\n",
    "After you load documents, you will often want to transform those documents to better suit your application.\n",
    "\n",
    "One of the most simple examples of making documents better suit your application is to split a long document into smaller chunks that can fit into your model's context window. LangChain has built-in document transformers that ease the process of splitting, combining, filtering, and otherwise manipulating documents.\n",
    "\n",
    "At a high level, here is how text splitters work:\n",
    "\n",
    "1. They split the text into small, semantically meaningful chunks (often sentences).\n",
    "2. They start combining these small chunks of text into a larger chunk until you reach a certain size (as measured by a specific function).\n",
    "3. After the combined text reaches the new chunk's size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap to keep context between chunks.\n",
    "\n",
    "For a list of types of text splitters LangChain supports, see [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950cce30",
   "metadata": {},
   "source": [
    "`CharacterTextSplitter`\n",
    "- Straightforward implementation\n",
    "- Consistent chunk sizes\n",
    "- Easily adaptable to different model requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "894245f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of chunks: 88\n"
     ]
    }
   ],
   "source": [
    "# Import the CharacterTextSplitter class from langchain_text_splitters module\n",
    "# Text splitters are used to divide large texts into smaller, manageable chunks\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Create a CharacterTextSplitter with specific configuration:\n",
    "# - chunk_size=200: Each chunk will contain approximately 200 characters\n",
    "# - chunk_overlap=20: Consecutive chunks will overlap by 20 characters to maintain context\n",
    "# - separator=\"\\n\": Text will be split at newline characters when possible\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")\n",
    "\n",
    "# Split the previously loaded document (PDF or other text) into chunks\n",
    "# The split_documents method:\n",
    "# 1. Takes a list of Document objects\n",
    "# 2. Splits each document's content based on the configured parameters\n",
    "# 3. Returns a new list of Document objects where each contains a chunk of text\n",
    "# 4. Preserves the original metadata for each chunk\n",
    "chunks = text_splitter.split_documents(document)\n",
    "\n",
    "# Print the total number of chunks created\n",
    "# This shows how many smaller Document objects were generated from the original document(s)\n",
    "# The number depends on the original document length and the chunk_size setting\n",
    "print(\"count of chunks: %d\" %len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9875d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"the metadata: {'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® \"\n",
      " \"Word 2016', 'creationdate': '2023-10-09T10:46:36+05:30', 'title': 'Paper \"\n",
      " \"Title (use style: paper title)', 'author': 'Keivalya Pandya;Dr Mehfuza \"\n",
      " \"Holia', 'moddate': '2023-10-09T10:46:36+05:30', 'source': \"\n",
      " \"'https://arxiv.org/pdf/2310.05421', 'total_pages': 4, 'page': 0, \"\n",
      " \"'page_label': '1'}\")\n",
      "('the page content: Submitted to the 3rd International Conference on “Women in '\n",
      " 'Science & Technology: Creating Sustainable Career”  \\n'\n",
      " '28 -30 December, 2023 \\n'\n",
      " 'Automating Customer Service using LangChain')\n"
     ]
    }
   ],
   "source": [
    "pprint(\"the metadata: %s\" % chunks[0].metadata)\n",
    "pprint(\"the page content: %s\" % chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d4da4",
   "metadata": {},
   "source": [
    " `RecursiveCharacterTextSplitter` implements this concept:\n",
    "- The RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\n",
    "- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n",
    "- This process continues down to the word level if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30615e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"the metadata: {'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® \"\n",
      " \"Word 2016', 'creationdate': '2023-10-09T10:46:36+05:30', 'title': 'Paper \"\n",
      " \"Title (use style: paper title)', 'author': 'Keivalya Pandya;Dr Mehfuza \"\n",
      " \"Holia', 'moddate': '2023-10-09T10:46:36+05:30', 'source': \"\n",
      " \"'https://arxiv.org/pdf/2310.05421', 'total_pages': 4, 'page': 0, \"\n",
      " \"'page_label': '1'}\")\n",
      "count of chunks: 89 \n",
      "\n",
      "('the page content: Submitted to the 3rd International Conference on “Women in '\n",
      " 'Science & Technology: Creating Sustainable Career”  \\n'\n",
      " '28 -30 December, 2023 \\n'\n",
      " 'Automating Customer Service using LangChain ')\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_recursive = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20, separators=[\"\\n\"])\n",
    "chunks_recursive = text_splitter_recursive.split_documents(document)\n",
    "pprint(\"the metadata: %s\" % chunks_recursive[0].metadata)\n",
    "print(\"count of chunks: %d \\n\" %len(chunks_recursive))\n",
    "pprint(\"the page content: %s \" % chunks_recursive[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5344c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Recursive Splitter ==============================\n",
      "the number of chunks created by Recursive Splitter: 33 \n",
      "\n",
      "the average characters in all chunks created by Recursive Splitter: 467\n",
      "\n",
      "the metadata keys in all chunks created by Recursive Splitter: dict_keys(['producer', 'creator', 'creationdate', 'title', 'author', 'moddate', 'source', 'total_pages', 'page', 'page_label'])\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): where the rhythms of mode rn life are guided by the pu lse of \n",
      "technology, the realm of customer service stands as the \n",
      "frontline of engagement betwee...\n",
      "Metadata: {'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-10-09T10:46:36+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'Keivalya Pandya;Dr Mehfuza Holia', 'moddate': '2023-10-09T10:46:36+05:30', 'source': 'https://arxiv.org/pdf/2310.05421', 'total_pages': 4, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 436 characters\n",
      "Max chunk size: 498 characters\n",
      "\n",
      "============================== Character Splitter ==============================\n",
      "the number of chunks created by Character Splitter: 33 \n",
      "\n",
      "the average characters in all chunks created by Character Splitter: 469\n",
      "\n",
      "the metadata keys in all chunks created by Character Splitter: dict_keys(['producer', 'creator', 'creationdate', 'title', 'author', 'moddate', 'source', 'total_pages', 'page', 'page_label'])\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): where the rhythms of mode rn life are guided by the pu lse of \n",
      "technology, the realm of customer service stands as the \n",
      "frontline of engagement betwee...\n",
      "Metadata: {'producer': 'Microsoft® Word 2016', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-10-09T10:46:36+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'Keivalya Pandya;Dr Mehfuza Holia', 'moddate': '2023-10-09T10:46:36+05:30', 'source': 'https://arxiv.org/pdf/2310.05421', 'total_pages': 4, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 436 characters\n",
      "Max chunk size: 498 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_chunks_splitter(text_splitter: RecursiveCharacterTextSplitter|CharacterTextSplitter,\n",
    "                            document:Document, name:str):\n",
    "    \n",
    "    chunks = text_splitter.split_documents(document)\n",
    "    \n",
    "    print(f\"the number of chunks created by {name}: {len(chunks)} \\n\")\n",
    "    avg_characters = sum(len(chunk.page_content) for chunk in chunks) / len(chunks)\n",
    "    print(f\"the average characters in all chunks created by {name}: {int(avg_characters)}\\n\")\n",
    "    \n",
    "    all_metadata_keys = chunks[0].metadata\n",
    "    print(f\"the metadata keys in all chunks  {all_metadata_keys.keys()}\\n\")\n",
    "\n",
    "    if chunks:\n",
    "        print(\"Example chunk:\")\n",
    "        example_doc = chunks[min(5, len(chunks)-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in chunks]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\\n\")\n",
    "\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=500, chunk_overlap=20, separator=\"\\n\")\n",
    "splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0, separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "\n",
    "print(\"=\"*30,\"Recursive Splitter\",\"=\"*30)\n",
    "display_chunks_splitter(text_splitter = splitter_2,document =document , name=\"Recursive Splitter\")\n",
    "\n",
    "print(\"=\"*30,\"Character Splitter\",\"=\"*30)\n",
    "display_chunks_splitter(text_splitter = splitter_1, document =document , name=\"Character Splitter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
